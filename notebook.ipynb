{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling\n",
    "\n",
    "Function calling within large language models (LLMs) allows the model to directly execute specific code modules or tools based on the user's instructions. This empowers the LLM to go beyond simply understanding keywords and towards performing tasks or retrieving information from the real world. In comparison to keyword-based approaches, where the LLM only generates text responses, function calling enables more tangible and accurate results, as it can directly interact with databases, APIs, and other external systems.\n",
    "\n",
    "Function calling and function usage were originally OpenAI technologies, but many models and backends integrate the necessary technologies to use this approach. Generally speaking, models need to be trained to guarantee better usage of tools because function calling requires the LLM to go beyond just recognizing words. It needs to grasp the user's intent to trigger a specific action or tool. Training helps the model learn to associate certain phrases, words, and sentence structures with a desire to execute a function.\n",
    "\n",
    "Here are some examples of existing datasets that can be used to teach a mistral or mixtral derivative model to execute those tasks:\n",
    "\n",
    "- [glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) by Glaive AI\n",
    "- [function_calling_v3](https://huggingface.co/datasets/Trelis/function_calling_v3) by Trelis (paid dataset)\n",
    "\n",
    "A better dataset can be built by using existing interaction data from your tools to identify messages where users are requesting image generation and feeding that data into a model to produce a synthetic dataset similar to the previously mentioned ones that is custom-tailored to your needs.\n",
    "\n",
    "## How to use\n",
    "\n",
    "To try this example, setup keys for Prodia and Anyscale in an .env file like:\n",
    "\n",
    "```\n",
    "PRODIA_API_KEY=...\n",
    "ANYSCALE_API_KEY=...\n",
    "```\n",
    "\n",
    "Install the requirements and run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "from langchain_core.pydantic_v1 import (\n",
    "    BaseModel,\n",
    "    Field,\n",
    "    ConfigDict,\n",
    "    SecretStr,\n",
    ")\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(BaseModel):\n",
    "    model_config: ConfigDict = ConfigDict()\n",
    "    max_tokens: int = Field(\n",
    "        description=\"The maximum number of tokens to generate.\",\n",
    "        default=2048,\n",
    "    )\n",
    "    temperature: float = Field(\n",
    "        description=\"The temperature to use for sampling.\",\n",
    "        default=0.7,\n",
    "        ge=0,\n",
    "    )\n",
    "    prodia_api_key: SecretStr = Field(\n",
    "        description=\"The Prodia API key.\",\n",
    "        default=SecretStr(environ[\"PRODIA_API_KEY\"]),\n",
    "    )\n",
    "    anyscale_api_key: SecretStr = Field(\n",
    "        description=\"The AnyScale API key.\",\n",
    "        default=SecretStr(environ[\"ANYSCALE_API_KEY\"]),\n",
    "    )\n",
    "    llm_model: str = Field(\n",
    "        description=\"The model to use for language generation.\",\n",
    "        default=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    )\n",
    "    sd_model: str = Field(\n",
    "        description=\"The model to use for image generation.\",\n",
    "        default=\"dreamshaperXL10_alpha2.safetensors [c8afe2ef]\",\n",
    "    )\n",
    "    sd_negative_prompt: str = Field(\n",
    "        description=\"A list of prompts to avoid when generating images.\",\n",
    "        default=\", \".join(open(\"negative_prompt.txt\").read().splitlines()),\n",
    "    )\n",
    "    sd_style_preset: str = Field(\n",
    "        description=\"The style preset to use for image generation.\",\n",
    "        default=\"photographic\",\n",
    "    )\n",
    "\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why langchain\n",
    "\n",
    "Langchain allows for quick prototyping of LLM agents and solutions, offering plenty of ready-to-use components that can be easily ported to a custom solution. Here we are using Langchain's tool framework to create a tool the language model can use. Our tool calls a Stable Diffusion model to genderate images according to the user's request. Prodia has been a stable provider in this field, and we use it for demonstration purposes, but the tool can call local sd models if required. This can also be run async, but the async implementation is missing here to make reading the notebook easier.\n",
    "\n",
    "As the tool output is fed back into the model both in case of success or failure, we must make sure the ToolException returns information the model can use to interact with the human in this loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prodiapy import Prodia\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_core.tools import ToolException\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "\n",
    "prodia = Prodia(\n",
    "    api_key=config.prodia_api_key.get_secret_value(),\n",
    ")\n",
    "\n",
    "\n",
    "class ImageGenerator(BaseTool):\n",
    "    \"\"\"\n",
    "    The ImageGenerator class generates an image based on the given prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    name: str | None = \"Image Generator\"\n",
    "    description: str | None = \"Generates an image based on the given prompt and returns the URL of the generated image.\"\n",
    "\n",
    "    def _run(\n",
    "        self, prompt: str, run_manager: CallbackManagerForToolRun | None = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generates an image based on the given prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The prompt for generating the image. A prompt is a text that provides instructions or\n",
    "                        guidance for generating the image. It can be a sentence, a paragraph, or even a short\n",
    "                        phrase. The generated image will be based on the content and context of the prompt.\n",
    "\n",
    "                        Examples of prompts:\n",
    "                        - \"Create a beautiful landscape with mountains and a sunset.\"\n",
    "                        - \"Design a futuristic cityscape with flying cars and skyscrapers.\"\n",
    "                        - \"Paint a portrait of a person with a mysterious expression.\"\n",
    "\n",
    "            run_manager (CallbackManagerForToolRun | None): An optional callback manager for tool run events.\n",
    "\n",
    "        Returns:\n",
    "            str: The URL of the generated image.\n",
    "\n",
    "        Raises:\n",
    "            ToolException: If there is an error generating the image.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            job = prodia.sdxl.generate(\n",
    "                prompt=prompt,\n",
    "                model=config.sd_model,\n",
    "                negative_prompt=config.sd_negative_prompt,\n",
    "                style_preset=config.sd_style_preset,\n",
    "            )\n",
    "            result = prodia.wait(job)\n",
    "\n",
    "            return str(result.image_url)\n",
    "        except Exception as e:\n",
    "            raise ToolException(\n",
    "                f\"Failed to generate image. Tell the user that you are sorry but you cannot show them a picture right now.\"\n",
    "            )\n",
    "\n",
    "    async def _arun(\n",
    "        self, query: str, run_manager: AsyncCallbackManagerForToolRun | None = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Use the tool asynchronously.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query string.\n",
    "            run_manager (AsyncCallbackManagerForToolRun | None): The run manager object.\n",
    "\n",
    "        Returns:\n",
    "            str: The result of the tool execution.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"ImageGenerator does not support async\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct Prompting\n",
    "\n",
    "ReAct prompting is a framework for interacting with large language models (LLMs) that combines reasoning and actions. It encourages the LLM to generate step-by-step reasoning traces leading up to a final answer, including the decision of when and which functions to call. Here we are using a simple ReAct prompt template I have hosted, but the prompt can be customized to be more tailored to your needs.\n",
    "\n",
    "Please note that mixing ReAct prompting with your existing prompt engineering strategies may prove difficult, and that tool usage will increase token generation but it is a safe and relatively trustworthy way to make models decide themselves when they need to generate an image.\n",
    "\n",
    "Other things could be plugged into this chain with relative ease, for example, if we wanted to keep a history of things the model showed an user we could have the `ImageGenerator` tool store the prompt into a database we can search afterwards whenever the user asks if the model remember something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.chat_models import ChatAnyscale\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "tools = [ImageGenerator()]\n",
    "\n",
    "prompt = hub.pull(\"afterst0rm/react-template\")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True, output_key=\"output\"\n",
    ")\n",
    "\n",
    "llm = ChatAnyscale(\n",
    "    api_key=config.anyscale_api_key.get_secret_value(),\n",
    "    model=config.llm_model,\n",
    "    max_tokens=config.max_tokens,\n",
    "    temperature=config.temperature,\n",
    ")\n",
    "\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    early_stopping_method=\"generate\",\n",
    "    handle_parsing_errors=True,\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model infers what it has to do\n",
    "\n",
    "As we can see in the execution below, the model itself is responsible for identifying when it has to generate an image, and our chain calls the function, feeding its output into the model. The tool is currently very simple and replies with a simple URL string. This can make the model confused sometimes, and the easiest way to solve issues that can come up from it is to use a structured output like a JSON where the field names are self-descriptive.\n",
    "\n",
    "Even with the lack of robustness in our prototype, the model successfully generates the image and answers to you with a link to it even without being directly asked for an image generation task. The ReAct prompt can be improved by analyzing anonymized chat history to find how users generally ask the model for pictures and changing the prompt accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m The user wants to see an image of two small cats playing with a ball of yarn. I can use the Image Generator tool to fulfill this request.\n",
      "\n",
      "Action: Image Generator\n",
      "Action Input: Two small cats playing with a ball of yarn\n",
      "\n",
      "Observ\u001b[0m\u001b[34mprodiapy\u001b[39m 19:44:49.601 - [\u001b[92mSUCCESS\u001b[39m]: Got result: {'job': '584da3b6-7a5e-4522-b4ca-dcd391ca7966', 'status': 'succeeded', 'imageUrl': 'https://images.prodia.xyz/584da3b6-7a5e-4522-b4ca-dcd391ca7966.png'}\n",
      "\u001b[36;1m\u001b[1;3mhttps://images.prodia.xyz/584da3b6-7a5e-4522-b4ca-dcd391ca7966.png\u001b[0m\u001b[32;1m\u001b[1;3m I have generated an image based on your description. Here is the URL for you to view it: https://images.prodia.xyz/584da3b6-7a5e-4\u001b[0mInvalid Format: Missing 'Action:' after 'Thought:\u001b[32;1m\u001b[1;3m It seems like there was an issue with the response formatting. I'll correct it and continue with the process.\n",
      "\n",
      "Thought: I have generated an image based on your description. Here is the URL for you to view it: https://images.prodia.xyz/584da3b6-7a5e-4522-b4ca-dcd391ca7966.png\n",
      "\n",
      "Final Answer: You can see the image of two small cats playing with a ball of yarn here: https://images.prodia.xyz/584da3b6-7a5e-4522-b4ca-dcd391ca7966.png\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I had a dream the other day where I saw two small cats playing with a ball of yarn. It was so cute! I wish I could have a cat like that or at least see something like that again.',\n",
       " 'chat_history': [],\n",
       " 'output': 'You can see the image of two small cats playing with a ball of yarn here: https://images.prodia.xyz/584da3b6-7a5e-4522-b4ca-dcd391ca7966.png',\n",
       " 'intermediate_steps': [(AgentAction(tool='Image Generator', tool_input='Two small cats playing with a ball of yarn\\n\\nObserv', log=' The user wants to see an image of two small cats playing with a ball of yarn. I can use the Image Generator tool to fulfill this request.\\n\\nAction: Image Generator\\nAction Input: Two small cats playing with a ball of yarn\\n\\nObserv'),\n",
       "   'https://images.prodia.xyz/584da3b6-7a5e-4522-b4ca-dcd391ca7966.png'),\n",
       "  (AgentAction(tool='_Exception', tool_input=\"Invalid Format: Missing 'Action:' after 'Thought:\", log=' I have generated an image based on your description. Here is the URL for you to view it: https://images.prodia.xyz/584da3b6-7a5e-4'),\n",
       "   \"Invalid Format: Missing 'Action:' after 'Thought:\")]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\"input\": \"I had a dream the other day where I saw two small cats playing with a ball of yarn. It was so cute! I wish I could have a cat like that or at least see something like that again.\"\n",
    "     }\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
